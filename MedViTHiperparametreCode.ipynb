{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOfAYKvvBaez4n28mMlrJD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berfingundem/BladderCancer/blob/main/MedViTHiperparametreCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu3OMxZvSzT1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from collections import Counter\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- GPU ayarÄ± ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"KullanÄ±lan cihaz:\", device)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Veri yolu ---\n",
        "data_dir = \"/content/drive/MyDrive/bladder_sett\"  # sadece HGC, LGC, NST, NTL klasÃ¶rlerini iÃ§ermeli\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "dataset = ImageFolder(data_dir, transform=transform)\n",
        "class_names = dataset.classes  # ['HGC','LGC','NST','NTL']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# --- Parametreler ---\n",
        "batch_size = 16\n",
        "num_epochs = 30\n",
        "k_folds = 5\n",
        "patience = 5  # early stopping\n",
        "\n",
        "# --- MedViT Modeli ---\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * 4, dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class MedViT(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            ConvBlock(3, 64),\n",
        "            ConvBlock(64, 128),\n",
        "            ConvBlock(128, 256)\n",
        "        )\n",
        "        self.flatten_size = 28\n",
        "        self.seq_len = self.flatten_size ** 2\n",
        "        self.embed_dim = 256\n",
        "        self.transformer = TransformerBlock(dim=self.embed_dim)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.seq_len * self.embed_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# --- K-Fold Cross Validation ---\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "fold_results = {}\n",
        "\n",
        "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
        "    print(f\"\\n--- Fold {fold+1}/{k_folds} ---\")\n",
        "\n",
        "    train_subsampler = Subset(dataset, train_ids)\n",
        "    val_subsampler = Subset(dataset, val_ids)\n",
        "\n",
        "    train_loader = DataLoader(train_subsampler, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_subsampler, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # --- Class Weights ---\n",
        "    targets = [dataset.targets[i] for i in train_ids]\n",
        "    class_counts = Counter(targets)\n",
        "    total = float(sum(class_counts.values()))\n",
        "    class_weights = [total / (num_classes * class_counts.get(i, 1)) for i in range(num_classes)]\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "    # --- Model ---\n",
        "    model = MedViT(num_classes=num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    best_val_loss = np.inf\n",
        "    patience_counter = 0\n",
        "\n",
        "    train_loss_list, val_loss_list = [], []\n",
        "    train_acc_list, val_acc_list = [], []\n",
        "\n",
        "    # --- EÄŸitim DÃ¶ngÃ¼sÃ¼ ---\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, correct, total_samples = 0, 0, 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = correct / total_samples\n",
        "\n",
        "        # --- DoÄŸrulama ---\n",
        "        model.eval()\n",
        "        val_loss, correct, total_samples = 0, 0, 0\n",
        "        y_true, y_pred, y_score = [], [], []\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # ðŸ”¹ val_lossâ€™u her batchâ€™te ekle\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "                y_pred.extend(preds.cpu().numpy())\n",
        "                y_score.extend(probs.cpu().numpy())\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total_samples += labels.size(0)\n",
        "\n",
        "        # ðŸ”¹ ortalama val_loss hesapla\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = correct / total_samples\n",
        "\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "              f\"Train Acc: {train_acc:.2f} | Val Acc: {val_acc:.2f}\")\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_wts = model.state_dict()\n",
        "            best_y_true, best_y_pred, best_y_score = y_true[:], y_pred[:], y_score[:]\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping tetiklendi.\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    fold_results[fold] = {\n",
        "        'train_acc': train_acc_list,\n",
        "        'val_acc': val_acc_list,\n",
        "        'train_loss': train_loss_list,\n",
        "        'val_loss': val_loss_list,\n",
        "        'y_true': best_y_true,\n",
        "        'y_pred': best_y_pred,\n",
        "        'y_score': np.array(best_y_score)\n",
        "    }\n",
        "\n",
        "    # --- Confusion Matrix ---\n",
        "    cm = confusion_matrix(best_y_true, best_y_pred, labels=list(range(num_classes)))\n",
        "    plt.figure(figsize=(6,5), dpi=300)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'Fold {fold+1} - Confusion Matrix')\n",
        "    plt.xlabel('Prediction'); plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # --- ROC Curve ---\n",
        "    y_true_bin = label_binarize(best_y_true, classes=list(range(num_classes)))\n",
        "    fpr, tpr, roc_auc = {}, {}, {}\n",
        "    for i in range(num_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], fold_results[fold]['y_score'][:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(7,6), dpi=300)\n",
        "    for i in range(num_classes):\n",
        "        plt.plot(fpr[i], tpr[i], label=f\"{class_names[i]} (AUC={roc_auc[i]:.2f})\")\n",
        "    plt.plot([0,1], [0,1], 'k--')\n",
        "    plt.title(f'Fold {fold+1} - ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# --- Fold ortalama doÄŸruluk ---\n",
        "all_val_acc = [max(fold_results[f]['val_acc']) for f in fold_results]\n",
        "print(\"\\nK-Fold ortalama doÄŸruluk:\", np.mean(all_val_acc))\n"
      ]
    }
  ]
}